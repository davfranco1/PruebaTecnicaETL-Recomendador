{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #2: Carga: creación de Base de Datos SQL\n",
    "\n",
    "En este segundo notebook crearemos y cargaremos nuestra base de datos.\n",
    "\n",
    "- El primer paso será importar las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías para tratamiento de datos\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "pd.set_option('display.max_columns', None) # Parámetro que modifica la visualización de los DFs\n",
    "\n",
    "# Librería para el acceso a variables y funciones\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import soporte_funciones as sf #Archivo .py donde encontraremos todas nuestras funciones.\n",
    "from src import soporte_variables as sv\n",
    "\n",
    "# Librería para trabajar con bases de datos SQL\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError, errorcodes, errors\n",
    "\n",
    "# Librería para ignorar avisos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignora TODOS los avisos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora importaremos los CSVs creados en el notebook anterior, y crearemos listas de tuplas, que es el formato necesario para la carga a la base de datos.\n",
    "- Para ello utilizaremos la función `sf.csvs_a_tuplas()`, que recibe como parámetros una lista con los directorios de los CSVs que queremos transformar, y devuelve una lista de tuplas para cada uno, que podemos asignar directamente a las variables.\n",
    "- Los geojson los trataremos más adelante, dado que requieren un tratamiento especial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutas = [\"../datos/finales/ingresos_hogares.csv\",\n",
    "\"../datos/finales/poblacion.csv\",\n",
    "\"../datos/finales/redpiso.csv\"]\n",
    "\n",
    "lista_ingresos, lista_poblacion, lista_redpiso = sf.csvs_a_tuplas(rutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El tercer paso consiste en crear la base de datos, para lo cual, usaremos la función `dbeaver_crear_db()`, que recibe como único argumento el nombre de la base de datos que deseamos crear. En su código, se crea la conexión a DBeaver, utilizando los parámetros de conexión (usuario y contraseña) que se han guardado en el soporte `../src/.env` (oculto) e importado en  `../src/soporte_funciones.py`.\n",
    "\n",
    "- Llamaremos a la base de datos \"AlquileresMadrid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos AlquileresMadrid creada con éxito\n"
     ]
    }
   ],
   "source": [
    "#sf.dbeaver_crear_db(\"alquileresmadrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los datos se insertarán en las tablas que crearemos a continuación, con ayuda de las queries de creación definidas en `../src/soporte_variables.py`.\n",
    "\n",
    "- Para crear esas tablas, hemos decidido que nuestra base de datos se estructurará de la manera en que vemos en el siguiente esquema entidad relación, que muestra la información que contiene cada tabla y cómo se relaciona entre sí. En el README del proyecto se explica su diseño.\n",
    "\n",
    "<img src=\"../images/DiagramaER.png\" width=\"400\">\n",
    "\n",
    "\n",
    "- En este caso utilizaremos dos funciones:\n",
    "\n",
    "    - `sf.dbeaver_conexion()`: recibe como parámetro el nombre de la base de datos de DBeaver y crea la conexión entre el notebook y la base de datos.\n",
    "    - `sf.dbeaver_commit()`: recibe como parámetros la conexión a DBeaver y la query de creación, realizando el commit hacia la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n",
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_distritos)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_airbnb)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_idealista)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_redpiso)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_ingreso_hogar)\n",
    "sf.dbeaver_commit(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_creacion_poblacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con las tablas preparadas, continuamos con la inserción de los mismos. Primero insertaremos manualmente las tablas que contienen información geoespacial, por necesitar un tratamiento especial, y luego seguiremos con los csv tradicionales.\n",
    "- Empezaremos por los importar los archivos de tipo `geojson` que se han creado en el notebook #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_distritos = gpd.read_file(\"../datos/finales/distritos.geojson\")\n",
    "gdf_airbnb = gpd.read_file(\"../datos/finales/airbnb.geojson\")\n",
    "gdf_idealista = gpd.read_file(\"../datos/finales/idealista.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por orden de inserción, debemos empezar por las tablas que sólo contienen Primary Keys, en este caso, distritos.\n",
    "- Usaremos la función de conexión a DBeaver `sf.dbeaver_conexion()`, que recibe como argumento el nombre de la base de datos, así como las queries de inserción que hemos definido en `src/soporte_variables.py`.\n",
    "- Debido a que cada DF tiene un formato distinto, no tiene sentido automatizar este proceso en una función.\n",
    "- Además de nuestras tuplas habituales para cada columna, en este caso, para las columnas que contienen datos geogmétricos, debemos exportarlas como `.wkt`. El formato WKT (Well Known Text), es un formato de codificación específicamente diseñado para la caracterización y almacenamiento de objetos geométricos espaciales en formato vectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a base de datos y cursor\n",
    "conn = sf.dbeaver_conexion(\"alquileresmadrid\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insertamos datos de cada GeoDataFrame:\n",
    "for _, fila in gdf_distritos.iterrows(): # Con la barra baja (_) indicamos que no queremos utilizar el índice del DF.\n",
    "    id_distrito = fila['ID_Distrito']\n",
    "    nombre = fila['Distrito']\n",
    "    geom = fila['geometry'].wkt\n",
    "\n",
    "    cur.execute(sv.query_inser_distritos, (id_distrito, nombre, geom))\n",
    "\n",
    "# Commit y cerrar\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para evitar errores cerramos el cursor y la conexión tras cada inserción. Seguimos con la tabla de AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a base de datos y cursor\n",
    "conn = sf.dbeaver_conexion(\"alquileresmadrid\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insertamos datos de cada GeoDataFrame:\n",
    "for _, fila in gdf_airbnb.iterrows():\n",
    "    id_distrito2 = fila['ID_Distrito']\n",
    "    precio2 = fila['Precio Total']\n",
    "    descripcion2 = fila['Descripcion']\n",
    "    geom2 = fila['geometry'].wkt\n",
    "    \n",
    "    cur.execute(sv.query_inser_airbnb, (id_distrito2, precio2, descripcion2, geom2))\n",
    "\n",
    "# Commit y cerrar\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente la tabla de Idealista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a base de datos y cursor\n",
    "conn = sf.dbeaver_conexion(\"alquileresmadrid\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insertamos datos de cada GeoDataFrame:\n",
    "for _, fila in gdf_idealista.iterrows():\n",
    "    id_distrito3 = fila['ID_Distrito']\n",
    "    precio3 = fila['Precio']\n",
    "    tipo3 = fila['Tipo']\n",
    "    planta3 = fila['Planta']\n",
    "    tamanio3 = fila['Tamanio']\n",
    "    habitaciones3 = fila['Habitaciones']\n",
    "    banios3 = fila['Banios']\n",
    "    direccion3 = fila['Direccion']\n",
    "    descripcion3 = fila['Descripcion']\n",
    "    geom3 = fila['geometry'].wkt\n",
    "    \n",
    "    cur.execute(sv.query_inser_idealista, (id_distrito3, precio3, tipo3, planta3, tamanio3, habitaciones3, banios3, direccion3, descripcion3, geom3))\n",
    "\n",
    "# Commit y cerrar\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Insertados los datos en las tablas que contienen datos de geolocalización, continuamos con las tres restantes: ingresos por hogar, población y Redpiso.\n",
    "\n",
    "- Usaremos la función `sf.dbeaver_commitmany()`, que recibe como argumentos:\n",
    "    - la conexión a DBeaver (usando la función de conexión),\n",
    "    - las queries de inserción que hemos definido en `src/soporte_variables.py` y,\n",
    "    - los datos que deseamos insertar, en este caso, las listas de tuplas preparadas anteriormente.\n",
    "- Esta función envía múltiples datos desde el notebook hacia la base de datos DBeaver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commitmany(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_inser_ingreso_hogar,lista_ingresos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commitmany(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_inser_poblacion,lista_poblacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit realizado\n"
     ]
    }
   ],
   "source": [
    "sf.dbeaver_commitmany(sf.dbeaver_conexion(\"alquileresmadrid\"),sv.query_inser_redpiso,lista_redpiso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con los datos insertados, podemos empezar a trabajar sobre nuestra base de datos. Continuamos con las consultas a la base de datos, la visualización y el análisis en el Notebook #3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
